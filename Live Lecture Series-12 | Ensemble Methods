#Week-12
# Video link: https://youtu.be/hCLKMiZBTrU

library(mlbench)
library(caret)
library(e1071)
library(lime)

# Boston Housing Data
data('BostonHousing')
mydata <- BostonHousing
set.seed(1234) 
ind <- sample(2, nrow(mydata), replace = T, prob = c(0.5, 0.5))
train <- mydata[ind == 1,]
test <- mydata[ind == 2,]

# Regression
# Bagging
set.seed(1234)
cvcontrol <- trainControl(method="repeatedcv", 
                          number = 5,
                          repeats = 2,
                          allowParallel=TRUE)
set.seed(1234)
bag <- train(medv ~ ., 
             data=train,
             method="treebag",
             trControl=cvcontrol,
             importance=TRUE)
plot(varImp(bag))

# Plot, RMSE, R-square
ba <- predict(bag,  test)
plot(ba ~ test$medv, main = 'Predicted Vs Actual MEDV - Test data')
sqrt(mean((test$medv - ba)^2))
cor(test$medv, ba) ^2

# RF
set.seed(1234)
forest <- train(medv ~ ., 
             data=train,
             method="rf",
             trControl=cvcontrol,
             importance=TRUE)
plot(varImp(forest))

# Plot, RMSE, R-square
rf <-  predict(forest,  test)
plot(rf ~ test$medv, main = 'Predicted Vs Actual MEDV - Test data')
sqrt(mean((test$medv - rf)^2))
cor(test$medv, rf) ^2

# Explain predictions
explainer <- lime(test[1:3,], forest, n_bins = 5)
explanation <- explain( x = test[1:3,], 
                       explainer = explainer, 
                       n_features = 5)
plot_features(explanation)

# Boosting
set.seed(1234)
boo <- train(medv ~ ., 
             data=train,
             method="xgbTree", 
             trControl=cvcontrol,
             tuneGrid = expand.grid(nrounds = 500,
                                    max_depth = 3,
                                    eta = 0.2,
                                    gamma = 2.1,
                                    colsample_bytree = 1,
                                    min_child_weight = 1,
                                    subsample = 1))
plot(varImp(boo))

# Plot, RMSE, R-square
bo <-  predict(boo,  test)
plot(bo ~ test$medv, main = 'Predicted Vs Actual MEDV - Test data')
sqrt(mean((test$medv - bo)^2))
cor(test$medv, bo) ^2

# Classification 
mydata <- read.csv('CTG.csv', header = T)
mydata$NSP <- as.factor(mydata$NSP)
set.seed(1234) 
ind <- sample(2, nrow(mydata), replace = T, prob = c(0.5, 0.5))
train <- mydata[ind == 1,]
test <- mydata[ind == 2,]

# Bagging
set.seed(1234) 
cvcontrol <- trainControl(method="repeatedcv", 
                          number = 5,
                          repeats = 1,
                          allowParallel=TRUE)
set.seed(1234)
bag <- train(NSP ~ ., 
             data=train,
             method="treebag",
             trControl=cvcontrol,
             importance=TRUE)
plot(varImp(bag))
p <- predict(bag, test, type = 'raw')
confusionMatrix(p, test$NSP)

# RF
set.seed(1234)
forest <- train(NSP ~ . , 
             data=train,
             method="rf",
             trControl=cvcontrol,
             importance=TRUE)
plot(varImp(forest))
p <- predict(forest, test, type = 'raw')
confusionMatrix(p, test$NSP)

# Boosting
set.seed(1234)
boo <- train(NSP ~ ., 
             data=train,
             method="xgbTree",   
             trControl=cvcontrol,
             tuneGrid = expand.grid(nrounds = 500,
                                    max_depth = 4,
                                    eta = 0.28,
                                    gamma = 1.8,
                                    colsample_bytree = 1,
                                    min_child_weight = 1,
                                    subsample = 1))
plot(varImp(boo))
p <- predict(boo, test, type = 'raw')
confusionMatrix(p, test$NSP)
